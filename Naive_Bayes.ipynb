{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"buy_computer_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.income.unique().size\n",
    "\n",
    "for i in df.columns:\n",
    "    display(i, df[i].unique().size, df[i].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043498af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_variable = \"buys_computer\"\n",
    "# if target_variable in df.columns:\n",
    "#     df[target_variable] = df[target_variable].str.lower()\n",
    "#     if  sorted(df[target_variable].unique()) == [\"no\", \"yes\"]:\n",
    "num_target_variable = df[target_variable].size\n",
    "\n",
    "yes_count = sum(df[target_variable] == \"yes\")\n",
    "no_count = sum(df[target_variable] == \"no\")\n",
    "target_variable_Percentage_Yes = yes_count / num_target_variable\n",
    "target_variable_Percent_No = no_count  / num_target_variable\n",
    "target_variable_Percent_No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\"yes\":{},\"no\":{} ,\"target_variable_Percentage_Yes\": yes_count / num_target_variable, \"target_variable_Percent_No\": no_count  / num_target_variable}\n",
    "\n",
    "for category in df:\n",
    "    if category == target_variable:\n",
    "        continue \n",
    "    \n",
    "    model[\"yes\"][category] = {}\n",
    "    model[\"no\"][category]= {}\n",
    "    \n",
    "    k = df[category].nunique()\n",
    "    \n",
    "    for parameter in df[category].unique():\n",
    "        df_parameter: pd.Series = df[category] == parameter\n",
    "        model[\"yes\"][category][parameter] = (sum((df_parameter) &  (df[target_variable] == \"yes\")) ) / (yes_count)\n",
    "        model[\"no\"][category][parameter] = (sum((df_parameter) &  (df[target_variable] == \"no\")) ) / (no_count )\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# buys_computer\tyes\t\n",
    "yes = 1\n",
    "no = 1\n",
    "p = {\"income\":\"high\",\"student\":\"no\", \"credit_rating\":\"fair\", \"age_group\":\"Very Young\"} \n",
    "for category in p:\n",
    "    yes *= model[\"yes\"][category][p[category]]\n",
    "    no *= model[\"no\"][category][p[category]]\n",
    "yes *= target_variable_Percentage_Yes\n",
    "no *= target_variable_Percent_No\n",
    "display(yes, no)\n",
    "r = \"yes\" if yes > no else \"no\"\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd21071",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = { \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises\", \"odor\",\n",
    "    \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\",\n",
    "    \"stalk-shape\", \"stalk-root\", \"stalk-surface-above-ring\", \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\", \"veil-color\",\n",
    "    \"ring-number\", \"ring-type\", \"spore-print-color\", \"population\", \"habitat\"\n",
    "\n",
    "b =  ['x','y','w','t','p',\"f\",\"c\",\"n\",\"n\",'e',\"e\",\"s\",'s',\"w\",\"w\",\"p\",\"w\",\"o\",\"p\",\"n\",\"s\",\"u\"]\n",
    "z = pd.Series(data=a, index=b)\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises\", \"odor\",\n",
    "    \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\",\n",
    "    \"stalk-shape\", \"stalk-root\", \"stalk-surface-above-ring\", \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\", \"veil-color\",\n",
    "    \"ring-number\", \"ring-type\", \"spore-print-color\", \"population\", \"habitat\"\n",
    "]\n",
    "\n",
    "values = [\n",
    "    \"x\", \"s\", \"n\", \"t\", \"p\", \"f\", \"c\", \"n\", \"k\", \"e\",\n",
    "    \"e\", \"s\", \"s\", \"w\", \"w\", \"p\", \"w\", \"o\", \"p\", \"k\", \"s\", \"u\"\n",
    "]\n",
    "\n",
    "# יצירת המילון\n",
    "result_dict = dict(zip(keys, values))\n",
    "\n",
    "# הדפסה לבדיקה\n",
    "print(result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3016ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"agaricus-lepiota.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8059b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in df.index:\n",
    "    print(df.iloc[i].to_dict())\n",
    "    # print(df.iloc[i,:].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6aa8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['e;mv;pdmnv', 'xlkdnvdkd', 'y', 'y', 't', 'l', 'f', 'c', 'b', 'g', 'e', 'c', 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'n', 'n', 'g']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f452f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['target'] = df.pop('target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c78b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "l ={\"s\":1}\n",
    "if l:\n",
    "    display(\"kk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"e;mv;pdmnv\"].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557130a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    \"yes\": {\n",
    "        \"income\": {\n",
    "            \"high\": 0.25,\n",
    "            \"medium\": 0.4166666666666667,\n",
    "            \"low\": 0.3333333333333333\n",
    "        },\n",
    "        \"student\": {\n",
    "            \"no\": 0.36363636363636365,\n",
    "            \"yes\": 0.6363636363636364\n",
    "        },\n",
    "        \"credit_rating\": {\n",
    "            \"fair\": 0.6363636363636364,\n",
    "            \"excellent\": 0.36363636363636365\n",
    "        },\n",
    "        \"age_group\": {\n",
    "            \"Very Young\": 0.25,\n",
    "            \"Young\": 0.4166666666666667,\n",
    "            \"Senior\": 0.3333333333333333\n",
    "        }\n",
    "    },\n",
    "    \"no\": {\n",
    "        \"income\": {\n",
    "            \"high\": 0.375,\n",
    "            \"medium\": 0.375,\n",
    "            \"low\": 0.25\n",
    "        },\n",
    "        \"student\": {\n",
    "            \"no\": 0.7142857142857143,\n",
    "            \"yes\": 0.2857142857142857\n",
    "        },\n",
    "        \"credit_rating\": {\n",
    "            \"fair\": 0.42857142857142855,\n",
    "            \"excellent\": 0.5714285714285714\n",
    "        },\n",
    "        \"age_group\": {\n",
    "            \"Very Young\": 0.5,\n",
    "            \"Young\": 0.125,\n",
    "            \"Senior\": 0.375\n",
    "        }\n",
    "    },\n",
    "    \"target_variable_Percentage_Yes\": 0.6428571428571429,\n",
    "    \"target_variable_Percent_No\": 0.35714285714285715\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a78025a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sms.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060a850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfad200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "מספר מילים חסרות משמעות באנגלית: 198\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')  # מוריד את רשימת המילים חסרות המשמעות\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"מספר מילים חסרות משמעות באנגלית:\", len(stop_words))\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in \"text text\".split() if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588e1e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m ss = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mCredit_Card_Applications.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ss[\u001b[33m'\u001b[39m\u001b[33mA7\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ss['A7'] = ss['A7'] +\"5\" \u001b[39;00m\n\u001b[32m      8\u001b[39m ss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10381\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10367\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10369\u001b[39m op = frame_apply(\n\u001b[32m  10370\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10371\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10379\u001b[39m     kwargs=kwargs,\n\u001b[32m  10380\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:248\u001b[39m, in \u001b[36m_coerce_method.<locals>.wrapper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m     warnings.warn(\n\u001b[32m    241\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on a single element Series is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdeprecated and will raise a TypeError in the future. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    246\u001b[39m     )\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m converter(\u001b[38;5;28mself\u001b[39m.iloc[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcannot convert the series to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ss = pd.read_csv(\"Credit_Card_Applications.csv\")\n",
    "\n",
    "ss['A7'] = ss.apply(int)\n",
    "\n",
    "ss['A7'] = ss['A7'] +\"5\" \n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac0a7931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\isaac/nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Naive_Bayes\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Naive_Bayes\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Naive_Bayes\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\isaac\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHe was running and studies showed he had studied it\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# פיצול למילים\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m words = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ניקוי stopwords (מילים חסרות משמעות כמו 'and', 'the' וכו')\u001b[39;00m\n\u001b[32m     20\u001b[39m filtered = [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\source\\repos\\Naive_Bayes\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\isaac/nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Naive_Bayes\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Naive_Bayes\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\isaac\\\\source\\\\repos\\\\Naive_Bayes\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\isaac\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "# להוריד פעם אחת\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# יצירת כלים\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"He was running and studies showed he had studied it\"\n",
    "\n",
    "# פיצול למילים\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# ניקוי stopwords (מילים חסרות משמעות כמו 'and', 'the' וכו')\n",
    "filtered = [w for w in words if w.lower() not in stopwords.words('english')]\n",
    "\n",
    "# Stemming\n",
    "stemmed = [stemmer.stem(w) for w in filtered]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "\n",
    "print(\"Original:\", filtered)\n",
    "print(\"Stemming:\", stemmed)\n",
    "print(\"Lemmatization:\", lemmatized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
